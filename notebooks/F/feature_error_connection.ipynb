{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85433c7a",
   "metadata": {},
   "source": [
    "### Feature-Error Relationship Analysis\n",
    "\n",
    "This notebook identifies which equation features are associated with specific MIRA extraction errors using:\n",
    "- Distance Correlation for detecting non-linear associations\n",
    "- Chi-square/Fisher's Tests for categorical feature-error relationships\n",
    "- Propensity Score Matching for establishing causality\n",
    "- Benjamini-Hochberg False Discovery Rate (FDR) for multiple testing correction\n",
    " \n",
    "\n",
    "Input: Features and categorized errors from error_analysis.ipynb\n",
    "\n",
    "Output: Ranked feature-error associations with statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5441b22",
   "metadata": {},
   "source": [
    "> tbcorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "711c2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29200c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "OUTPUT_DIR = './feature_error_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158312e",
   "metadata": {},
   "source": [
    "Creating features dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fd0509",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 130) (2735612189.py, line 130)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31manalysis_errors_df.loc[idx, 'derivative_\u001b[39m\n                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 130)\n"
     ]
    }
   ],
   "source": [
    "#load features data\n",
    "features_df = pd.read_csv('test_features.csv')\n",
    "print(f\"Features loaded: {features_df.shape}\")\n",
    "print(f\"Models in features: {features_df['model'].unique()}\")\n",
    "\n",
    "# Step 2: Create function to load all categorization files\n",
    "def load_all_categorizations(pattern='*_categorization.json'):\n",
    "    \"\"\"\n",
    "    Load all categorization JSON files and create errors dataframe\n",
    "    \"\"\"\n",
    "    categorizations = {}\n",
    "    \n",
    "    # If pattern doesn't find files, try specific file\n",
    "    json_files = glob.glob(pattern)\n",
    "    if not json_files:\n",
    "        json_files = ['test_categorization.json']\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                model = data['model']\n",
    "                categorizations[model] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return categorizations\n",
    "\n",
    "# Step 3: Convert categorizations to errors dataframe\n",
    "def create_errors_dataframe(categorizations_dict, features_df):\n",
    "    \"\"\"\n",
    "    Create binary errors dataframe from categorization results\n",
    "    \"\"\"\n",
    "    # Define all possible error types from MIRA\n",
    "    error_types = [\n",
    "        'symbol_recognition',\n",
    "        'subscript_superscript',\n",
    "        'structural_corruption',\n",
    "        'derivative_notation',\n",
    "        'boundary_initial_conditions',\n",
    "        'operator_errors',\n",
    "        'completeness_errors',\n",
    "        'formatting_errors'\n",
    "    ]\n",
    "    \n",
    "    errors_data = []\n",
    "    \n",
    "    # Process each model in features\n",
    "    for model in features_df['model'].unique():\n",
    "        error_row = {'model': model}\n",
    "        \n",
    "        if model in categorizations_dict:\n",
    "            cat_data = categorizations_dict[model]\n",
    "            errors_by_cat = cat_data['error_distribution']['by_category']\n",
    "            \n",
    "            # Set binary values for each error type\n",
    "            for error_type in error_types:\n",
    "                # 1 if error exists, 0 otherwise\n",
    "                error_row[error_type] = 1 if errors_by_cat.get(error_type, 0) > 0 else 0\n",
    "            \n",
    "            # Also store severity for additional analysis\n",
    "            error_row['overall_severity'] = cat_data.get('overall_severity', 'none')\n",
    "            error_row['extraction_quality'] = cat_data.get('extraction_quality_score', 100)\n",
    "            \n",
    "        else:\n",
    "            # Model not found in categorizations - assume no errors\n",
    "            print(f\"Warning: Model {model} not found in categorizations\")\n",
    "            for error_type in error_types:\n",
    "                error_row[error_type] = 0\n",
    "            error_row['overall_severity'] = 'none'\n",
    "            error_row['extraction_quality'] = 100\n",
    "        \n",
    "        errors_data.append(error_row)\n",
    "    \n",
    "    return pd.DataFrame(errors_data)\n",
    "\n",
    "# Step 4: Load and process data\n",
    "categorizations = load_all_categorizations()\n",
    "print(f\"\\nLoaded categorizations for {len(categorizations)} models\")\n",
    "\n",
    "errors_df = create_errors_dataframe(categorizations, features_df)\n",
    "print(f\"\\nErrors dataframe created: {errors_df.shape}\")\n",
    "\n",
    "# Step 5: Prepare data for analysis\n",
    "# Remove non-feature columns from features_df\n",
    "feature_cols = [col for col in features_df.columns if col not in ['model', 'original_equation']]\n",
    "analysis_features_df = features_df[feature_cols]\n",
    "\n",
    "# Get only error columns for analysis (exclude model, severity, quality)\n",
    "error_cols = [col for col in errors_df.columns if col not in ['model', 'overall_severity', 'extraction_quality']]\n",
    "analysis_errors_df = errors_df[error_cols]\n",
    "\n",
    "print(f\"\\nPrepared for analysis:\")\n",
    "print(f\"Features: {analysis_features_df.shape} - {len(feature_cols)} features\")\n",
    "print(f\"Errors: {analysis_errors_df.shape} - {len(error_cols)} error types\")\n",
    "\n",
    "# Step 6: Check data\n",
    "print(\"\\nData Summary:\")\n",
    "print(f\"Total models: {len(features_df)}\")\n",
    "print(f\"Models with errors: {(analysis_errors_df.sum(axis=1) > 0).sum()}\")\n",
    "print(f\"\\nError type frequencies:\")\n",
    "print(analysis_errors_df.sum())\n",
    "\n",
    "# Step 7: Handle case where all extractions are perfect\n",
    "if analysis_errors_df.sum().sum() == 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: All extractions are perfect (no errors found)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nFor meaningful feature-error analysis, you need models with errors.\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Run error analysis on more models that have extraction errors\")\n",
    "    print(\"2. Use synthetic error data for testing the analysis pipeline\")\n",
    "    \n",
    "    # Create synthetic errors for demonstration\n",
    "    create_synthetic = input(\"\\nCreate synthetic errors for demonstration? (y/n): \")\n",
    "    \n",
    "    if create_synthetic.lower() == 'y':\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create synthetic errors based on feature complexity\n",
    "        for idx, row in features_df.iterrows():\n",
    "            # Higher complexity → more likely errors\n",
    "            if row['overall_complexity_score'] > 6:\n",
    "                analysis_errors_df.loc[idx, 'symbol_recognition'] = 1\n",
    "            if row['has_nested_subscripts']:\n",
    "                analysis_errors_df.loc[idx, 'subscript_superscript'] = 1\n",
    "            if row['unicode_ratio'] > 0.2:\n",
    "                analysis_errors_df.loc[idx, 'operator_errors'] = 1\n",
    "            if row['mixed_derivative_notation']:\n",
    "                analysis_errors_df.loc[idx, 'derivative_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecd310",
   "metadata": {},
   "source": [
    "#### Distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35140654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_correlation(x, y):\n",
    "\n",
    "    x = np.atleast_1d(x)\n",
    "    y = np.atleast_1d(y)\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"x and y must have the same length\")\n",
    "    \n",
    "    n = len(x)\n",
    "    if n < 2:\n",
    "        return 0\n",
    "    \n",
    "    # Center the distance matrices\n",
    "    x = x.reshape(-1, 1) if x.ndim == 1 else x\n",
    "    y = y.reshape(-1, 1) if y.ndim == 1 else y\n",
    "    \n",
    "    # Compute distance matrices\n",
    "    a = squareform(pdist(x))\n",
    "    b = squareform(pdist(y))\n",
    "    \n",
    "    # Double center the distance matrices\n",
    "    a_mean_rows = a.mean(axis=1, keepdims=True)\n",
    "    a_mean_cols = a.mean(axis=0, keepdims=True)\n",
    "    a_mean_all = a.mean()\n",
    "    a_centered = a - a_mean_rows - a_mean_cols + a_mean_all\n",
    "    \n",
    "    b_mean_rows = b.mean(axis=1, keepdims=True)\n",
    "    b_mean_cols = b.mean(axis=0, keepdims=True)\n",
    "    b_mean_all = b.mean()\n",
    "    b_centered = b - b_mean_rows - b_mean_cols + b_mean_all\n",
    "    \n",
    "    # Calculate distance covariance and variances\n",
    "    dcov_squared = (a_centered * b_centered).sum() / (n * n)\n",
    "    dvar_x_squared = (a_centered * a_centered).sum() / (n * n)\n",
    "    dvar_y_squared = (b_centered * b_centered).sum() / (n * n)\n",
    "    \n",
    "    # Calculate distance correlation\n",
    "    if dvar_x_squared * dvar_y_squared > 0:\n",
    "        dcor = np.sqrt(dcov_squared) / np.sqrt(np.sqrt(dvar_x_squared) * np.sqrt(dvar_y_squared))\n",
    "    else:\n",
    "        dcor = 0\n",
    "    \n",
    "    return min(max(dcor, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distance_correlations(features_df, errors_df):\n",
    "    \"\"\"\n",
    "    Distance Correlation for detecting non-linear associations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    numeric_features = features_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for feature_col in numeric_features:\n",
    "        for error_col in errors_df.columns:\n",
    "            feature_data = features_df[feature_col].values\n",
    "            error_data = errors_df[error_col].astype(int).values\n",
    "            \n",
    "            # Skip if no variance\n",
    "            if np.std(feature_data) == 0 or np.std(error_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate distance correlation\n",
    "            dcor = distance_correlation(feature_data, error_data)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature_col,\n",
    "                'error_type': error_col,\n",
    "                'method': 'distance_correlation',\n",
    "                'statistic': dcor,\n",
    "                'interpretation': 'strength of non-linear association (0-1)'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661c0d9",
   "metadata": {},
   "source": [
    "### Multiple methods to capture different types of relationships between features and errors  (using sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2fe51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     48\u001b[39m     ax.set_title(\u001b[33m'\u001b[39m\u001b[33mFeature Importance Across Different Methods (Normalized)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m results, ensemble_scores = comprehensive_sklearn_analysis(\u001b[43mfeatures_df\u001b[49m, errors_df)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# DISPLAY RESULTS\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'features_df' is not defined"
     ]
    }
   ],
   "source": [
    "def comprehensive_sklearn_analysis(features_df, errors_df):\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # 1. Mutual Information (non-linear relationships)\n",
    "    print(\"1. Mutual Information Analysis...\")\n",
    "    mi_results = analyze_mutual_info_sklearn(features_df, errors_df)\n",
    "    all_results.append(mi_results)\n",
    "    \n",
    "    # 2. Random Forest (captures interactions automatically)\n",
    "    print(\"2. Random Forest Importance...\")\n",
    "    rf_results = analyze_rf_importance(features_df, errors_df)\n",
    "    all_results.append(rf_results)\n",
    "    \n",
    "    # 3. Permutation Importance (model-agnostic)\n",
    "    print(\"3. Permutation Importance...\")\n",
    "    perm_results = analyze_permutation_importance(features_df, errors_df)\n",
    "    all_results.append(perm_results)\n",
    "    \n",
    "    # Combine all\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Create ensemble score (average rank across methods)\n",
    "    combined['rank'] = combined.groupby(['method'])['statistic'].rank(ascending=False)\n",
    "    ensemble_scores = combined.groupby(['feature', 'error_type'])['rank'].mean()\n",
    "    \n",
    "    return combined, ensemble_scores\n",
    "\n",
    "# plot\n",
    "def visualize_method_comparison(results_df):\n",
    "    \"\"\"\n",
    "    Compare different methods' findings\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # pivot for heatmap\n",
    "    pivot = results_df.pivot_table(\n",
    "        index='feature',\n",
    "        columns='method',\n",
    "        values='statistic',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # normalize each method to 0-1 for comparison\n",
    "    pivot_norm = (pivot - pivot.min()) / (pivot.max() - pivot.min())\n",
    "    \n",
    "    sns.heatmap(pivot_norm, annot=True, fmt='.2f', cmap='viridis', ax=ax)\n",
    "    ax.set_title('Feature Importance Across Different Methods (Normalized)')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "results, ensemble_scores = comprehensive_sklearn_analysis(features_df, errors_df)\n",
    "\n",
    "# DISPLAY RESULTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP FEATURE-ERROR RELATIONSHIPS (by ensemble score)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show top 15 relationships\n",
    "top_relationships = ensemble_scores.sort_values().head(15)\n",
    "for (feature, error), rank in top_relationships.items():\n",
    "    print(f\"{feature} → {error}: average rank = {rank:.2f}\")\n",
    "\n",
    "# Show top findings by each method\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP FINDINGS BY METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in results['method'].unique():\n",
    "    print(f\"\\nTop 5 for {method}:\")\n",
    "    method_results = results[results['method'] == method].nlargest(5, 'statistic')\n",
    "    for _, row in method_results.iterrows():\n",
    "        print(f\"  {row['feature']} → {row['error_type']}: {row['statistic']:.4f}\")\n",
    "\n",
    "# SHOW THE PLOT\n",
    "fig = visualize_method_comparison(results)\n",
    "plt.show()\n",
    "\n",
    "# Additional summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total feature-error pairs analyzed: {len(results) // 3}\")  # divided by 3 methods\n",
    "print(f\"Average importance across all methods: {results['statistic'].mean():.4f}\")\n",
    "print(f\"Methods analyzed: {', '.join(results['method'].unique())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "mira_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
