{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b0cf2f",
   "metadata": {},
   "source": [
    "# Workflow for MIRA equations extractions - notes\n",
    "\n",
    "This notebook outlines ideas for prompting for extracting mathematical equations from PDFs in the MIRA framework.\n",
    "\n",
    "---\n",
    "\n",
    "## One-shot Prompting:\n",
    "The basic workflow of MIRA is a one-shot prompting architecture (let's call this **verison = 001**).\n",
    "\n",
    "\n",
    "Process of the extraction: *'mira/notebooks/llm_extraction.ipynb'*\n",
    "\n",
    "Pipeline: *'mira/sources/sympy_ode/llm_util.py'\n",
    "\n",
    "Prompts: *'mira/sources/sympy_ode/constants.py'*\n",
    "\n",
    "Detailed results can be found in the notebook: *mira_llm_extraction_evaluation.ipynb*\n",
    "\n",
    "---\n",
    "\n",
    "## Iterative promting workflow:\n",
    "**version = 002**\n",
    "\n",
    "To improve the precision of the extraction, an iterative workflow is being introduced, with the following steps:\n",
    "### Agent 1:\n",
    "First, an extraction agent uses the original MIRA process to convert equation images into SymPy code and ground biological concepts. \n",
    "\n",
    "### Agent 2:\n",
    "Then, a validation agent checks the extraction for execution errors (missing imports, undefined variables), parameter consistency issues, and incorrect concept grounding. \n",
    "If errors are found, the validation agent corrects them and the process repeats for up to 3 iterations until all checks pass. \n",
    "\n",
    "This multi-agent approach improves extraction accuracy by catching and fixing common errors that the single-shot method might miss, while maintaining backward compatibility with the existing MIRA codebase.\n",
    "\n",
    "### RESULTS OF IMPLEMENTATION:\n",
    "*Forked version on GitHub: *'fruzsedua/mira/tree/extraction-development'*\n",
    "\n",
    "Examples for each result found in this folder: *'mira/notebooks/equation extraction development/extraction error check/string mismatch check/comparison_results_version002'*\n",
    "\n",
    "Process of the extraction: *'mira/notebooks/llm_extraction.ipynb'* -> **More detalied process**\n",
    "\n",
    "Pipeline: *'mira/sources/sympy_ode/llm_util.py' -> **New functions added**\n",
    "\n",
    "Prompts: *'mira/sources/sympy_ode/constants.py'* -> **Error handling prompt added**\n",
    "**\n",
    "**Image extraction:**\n",
    "- Additional rules added: symmetry, transmission structure, patterns, mathematical structure, parameter consistency, completeness check\n",
    "- Epidemology based rules are just ideas (from Claude) -> *revision needed!*\n",
    "\n",
    "**Error checking and correcting:**\n",
    "- Execution errors are mostly fixed during iteration 1:\n",
    "- Syntax rules for detecting and handling functions/symbols\n",
    "- Handling of imports, utilizing their names precisely\n",
    "- Missing parameters are included\n",
    "\n",
    "- Data cannot be parsed if the output format of the prompt is not aligned with the next function -> exact clarification is added to the prompt\n",
    "- Comparing number of factors to the original (count * operators and variables)\n",
    "- Preserving content between iterations of the error handling prompt\n",
    "- Missing /N fixed\n",
    "\n",
    "**Comparison of the extracted odes added:**\n",
    "- Sympy format matching\n",
    "- Sorting of equations (based on the variable on the LHS) for comparison\n",
    "- Template Model → Mtx odes confuses a lot of information due to multiple formatting steps -> *fix needed!*\n",
    "\n",
    "\n",
    "Error handling multi-agent architecture is part of the tm creation \n",
    "pipeline:\n",
    "\n",
    "**Image → LLM Extraction → Multi-Agent Validation → JSON (corrected ODEs + concepts) → Template Model → Mtx odes**\n",
    "\n",
    "**REMAINING ERRORS:**\n",
    "- Parameter consistency: mostly symbolic differences (e.g. rho_1 vs. rho1), sometimes more serious: e.g. rho vs. q (similar) -> LLM has no info, which one is used, doesn’t know it needs fix\n",
    "- Multiplication vs. addition still gets mixed up sometimes\n",
    "- Semantic compartment mismatches I(t) vs. T(t)-> extra validation needed e.g. linear and \n",
    "- Strengthening of the arithmetic validation is much needed!\n",
    "- Precision of coefficient extraction \n",
    "- Still remains: CodeExecutionError: Error while executing the code: 'Symbol' object is not callable (examples: BIOMD000000972, BIOMD000000976)\n",
    "- The error handling function  mixes up the order of operations in some cases (example: BIOMD0000000991)\n",
    "- Extraction of the compartments differ from the original completely, maybe derived from the RHS (example: 2024_dec_epi_1_model_A)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> NEXT STEP:\n",
    "## Multi-Agent Pipeline:\n",
    "**version= 003**\n",
    "\n",
    "There are clearly separable problem areas, which will be better managed by detailing and resolving the prompting. An agenda based approach will systematically address extraction challenges by organizing the process into distinct agenda items, each targeting specific aspects of the process:\n",
    "\n",
    "### Agent 1: Initial Extraction\n",
    "- Extract equations from image/PDF using existing MIRA logic\n",
    "- Convert mathematical notation to SymPy code representation\n",
    "- Pass raw code string to next agent\n",
    "\n",
    "### Agent 2: Execution Error Handler\n",
    "- Attempt to execute the extracted SymPy code\n",
    "- Catch and diagnose execution errors (missing imports, undefined variables, syntax errors)\n",
    "- Automatically fix common issues and retry execution\n",
    "- Pass executable code and any remaining warnings forward\n",
    "\n",
    "### Agent 3: Symbol & Parameter Analysis\n",
    "#### Time dependency classification:\n",
    "- Identify all variables that appear with d/dt (time-dependent)\n",
    "- Classify remaining symbols as parameters or independent variables\n",
    "- Flag any inconsistencies in variable usage\n",
    "\n",
    "#### Parameter consistency checking:\n",
    "- Detect parameters that appear in equations but aren't defined\n",
    "- Identify duplicate parameter definitions\n",
    "- Find defined but unused parameters\n",
    "- Check notation consistency (subscripts, superscripts, Greek letters)\n",
    "- Pass comprehensive symbol mapping to next agent\n",
    "\n",
    "### Agent 4: Diagnostic & Scoring\n",
    "- Calculate extraction quality score based on:\n",
    " - Successful execution (from Agent 2)\n",
    " - Symbol consistency (from Agent 3)\n",
    " - Common extraction error patterns\n",
    "- Generate final report with:\n",
    " - Overall confidence score\n",
    " - Specific warnings about potential extraction errors\n",
    " - Recommendations for manual review if score is low\n",
    "- Optional: Include lightweight mathematical validation (missing negative signs, suspicious parameter usage)\n",
    "\n",
    "This pipeline transforms the single-shot extraction into a robust, multi-step process where each agent specializes in one aspect of validation and correction. Since each agent requires a distinct approach and prompt configuration, the LLM can achieve better focus (rather than receiving a summarized, less detailed message).\n",
    "\n",
    "Other possible agenda items:\n",
    "\n",
    "6. Symbol Validation – Are all variables and parameters defined? this focuses more on JSON\n",
    "\n",
    "7. Biological Context Tagging – Are compartments semantically labeled (e.g., S = susceptible)?\n",
    "\n",
    "8. JSON Structure Integrity – Is the output JSON consistent and complete?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c25dc",
   "metadata": {},
   "source": [
    "### Quantitative measures for evaluating the extraction process\n",
    "> TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class ODEsEvaluator:\n",
    "    \n",
    "    def execution_success_rate(self, odes_list):\n",
    "        \"\"\"\n",
    "        Check if ODEs can be executed without errors.\n",
    "        \"\"\"\n",
    "        total = len(odes_list)\n",
    "        successful = 0\n",
    "        failed = []\n",
    "        \n",
    "        for i, eq in enumerate(odes_list):\n",
    "            try:\n",
    "                # Check if it's a valid SymPy equation\n",
    "                if hasattr(eq, 'lhs') and hasattr(eq, 'rhs'):\n",
    "                    # Try to evaluate/simplify\n",
    "                    sympy.simplify(eq.lhs - eq.rhs)\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed.append(i)\n",
    "            except Exception as e:\n",
    "                failed.append(i)\n",
    "        \n",
    "        return {\n",
    "            'success_rate': successful / total if total > 0 else 0,\n",
    "            'successful': successful,\n",
    "            'failed': len(failed),\n",
    "            'total': total,\n",
    "            'failed_indices': failed\n",
    "        }\n",
    "    \n",
    "    def symbol_accuracy(self, extracted_eq, correct_eq):\n",
    "        \"\"\"\n",
    "        Compare symbols between extracted and correct equations.\n",
    "        \"\"\"\n",
    "        # Get all symbols from equations\n",
    "        extracted_symbols = set(str(s) for s in extracted_eq.free_symbols)\n",
    "        correct_symbols = set(str(s) for s in correct_eq.free_symbols)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        if len(correct_symbols) == 0:\n",
    "            return 1.0 if len(extracted_symbols) == 0 else 0.0\n",
    "        \n",
    "        correct_matches = extracted_symbols & correct_symbols\n",
    "        missing = correct_symbols - extracted_symbols\n",
    "        extra = extracted_symbols - correct_symbols\n",
    "        \n",
    "        accuracy = len(correct_matches) / len(correct_symbols)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'missing': list(missing),\n",
    "            'extra': list(extra),\n",
    "            'correct_count': len(correct_matches),\n",
    "            'total_expected': len(correct_symbols)\n",
    "        }\n",
    "    \n",
    "    def check_mathematical_equivalence(self, extracted_eq, correct_eq):\n",
    "        \"\"\"\n",
    "        Check if two equations are mathematically equivalent.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate difference\n",
    "            diff = sympy.simplify((extracted_eq.lhs - extracted_eq.rhs) - \n",
    "                                (correct_eq.lhs - correct_eq.rhs))\n",
    "            \n",
    "            # Check if difference is zero (equations are equivalent)\n",
    "            is_equivalent = diff == 0\n",
    "            \n",
    "            return {\n",
    "                'equivalent': is_equivalent,\n",
    "                'difference': str(diff),\n",
    "                'severity': 0 if is_equivalent else self.calculate_severity(diff)\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'equivalent': False,\n",
    "                'difference': 'Could not compute',\n",
    "                'severity': 1.0\n",
    "            }\n",
    "    \n",
    "    def calculate_severity(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate error severity based on the difference.\n",
    "        \"\"\"\n",
    "        diff_str = str(diff)\n",
    "        \n",
    "        # If difference is 0, no error\n",
    "        if diff == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Count terms in difference\n",
    "        terms = diff_str.count('+') + diff_str.count('-') + 1\n",
    "        \n",
    "        # More terms = more severe\n",
    "        if terms == 1:\n",
    "            return 0.3  # Single term difference\n",
    "        elif terms == 2:\n",
    "            return 0.5  # Two terms difference\n",
    "        else:\n",
    "            return 0.8  # Many terms difference\n",
    "    \n",
    "    def evaluate_odes_set(self, extracted_odes, correct_odes, set_name=\"Extracted\"):\n",
    "        \"\"\"\n",
    "        Evaluate a complete set of ODEs against correct ODEs.\n",
    "        \"\"\"\n",
    "        # Execution success\n",
    "        exec_results = self.execution_success_rate(extracted_odes)\n",
    "        \n",
    "        # Compare each equation\n",
    "        equation_results = []\n",
    "        total_accuracy = 0\n",
    "        total_equivalent = 0\n",
    "        total_severity = 0\n",
    "        \n",
    "        for i, (extracted, correct) in enumerate(zip(extracted_odes, correct_odes)):\n",
    "            # Symbol accuracy\n",
    "            symbol_result = self.symbol_accuracy(extracted, correct)\n",
    "            \n",
    "            # Mathematical equivalence\n",
    "            equiv_result = self.check_mathematical_equivalence(extracted, correct)\n",
    "            \n",
    "            equation_results.append({\n",
    "                'index': i,\n",
    "                'symbol_accuracy': symbol_result['accuracy'],\n",
    "                'equivalent': equiv_result['equivalent'],\n",
    "                'severity': equiv_result['severity']\n",
    "            })\n",
    "            \n",
    "            total_accuracy += symbol_result['accuracy']\n",
    "            total_equivalent += 1 if equiv_result['equivalent'] else 0\n",
    "            total_severity += equiv_result['severity']\n",
    "        \n",
    "        n = len(correct_odes)\n",
    "        \n",
    "        return {\n",
    "            'set_name': set_name,\n",
    "            'execution_success_rate': exec_results['success_rate'],\n",
    "            'avg_symbol_accuracy': total_accuracy / n if n > 0 else 0,\n",
    "            'exact_match_rate': total_equivalent / n if n > 0 else 0,\n",
    "            'avg_severity': total_severity / n if n > 0 else 0,\n",
    "            'quality_score': 1 - (total_severity / n) if n > 0 else 0,\n",
    "            'equation_details': equation_results,\n",
    "            'summary': self.get_summary(exec_results['success_rate'], \n",
    "                                       total_equivalent / n if n > 0 else 0)\n",
    "        }\n",
    "    \n",
    "    def get_summary(self, exec_rate, match_rate):\n",
    "        \"\"\"\n",
    "        Get qualitative summary.\n",
    "        \"\"\"\n",
    "        score = (exec_rate + match_rate) / 2\n",
    "        if score >= 0.9:\n",
    "            return \"Excellent\"\n",
    "        elif score >= 0.7:\n",
    "            return \"Good\"\n",
    "        elif score >= 0.5:\n",
    "            return \"Fair\"\n",
    "        else:\n",
    "            return \"Needs Improvement\"\n",
    "    \n",
    "    def compare_all_methods(self, correct_odes_sorted, extracted_odes_sorted, \n",
    "                           corrected_odes_sorted, mtx_odes_sorted):\n",
    "        \"\"\"\n",
    "        Compare all extraction methods against correct ODEs.\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"MIRA EQUATION EXTRACTION EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Evaluate extracted_odes\n",
    "        print(\"\\n1. EXTRACTED ODEs EVALUATION\")\n",
    "        print(\"-\"*40)\n",
    "        extracted_results = self.evaluate_odes_set(\n",
    "            extracted_odes_sorted, correct_odes_sorted, \"Extracted\"\n",
    "        )\n",
    "        results['extracted'] = extracted_results\n",
    "        self.print_results(extracted_results)\n",
    "        \n",
    "        # 2. Evaluate corrected_odes\n",
    "        print(\"\\n2. CORRECTED ODEs EVALUATION\")\n",
    "        print(\"-\"*40)\n",
    "        corrected_results = self.evaluate_odes_set(\n",
    "            corrected_odes_sorted, correct_odes_sorted, \"Corrected\"\n",
    "        )\n",
    "        results['corrected'] = corrected_results\n",
    "        self.print_results(corrected_results)\n",
    "        \n",
    "        # 3. Evaluate mtx_odes\n",
    "        print(\"\\n3. MATRIX ODEs EVALUATION\")\n",
    "        print(\"-\"*40)\n",
    "        # Handle potential length mismatch\n",
    "        min_len = min(len(mtx_odes_sorted), len(correct_odes_sorted))\n",
    "        mtx_results = self.evaluate_odes_set(\n",
    "            mtx_odes_sorted[:min_len], correct_odes_sorted[:min_len], \"Matrix\"\n",
    "        )\n",
    "        results['matrix'] = mtx_results\n",
    "        self.print_results(mtx_results)\n",
    "        \n",
    "        # Overall comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Method':<15} {'Exec Rate':<12} {'Match Rate':<12} {'Quality':<12} {'Assessment':<12}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for method in ['extracted', 'corrected', 'matrix']:\n",
    "            r = results[method]\n",
    "            print(f\"{method.capitalize():<15} \"\n",
    "                  f\"{r['execution_success_rate']:<12.1%} \"\n",
    "                  f\"{r['exact_match_rate']:<12.1%} \"\n",
    "                  f\"{r['quality_score']:<12.1%} \"\n",
    "                  f\"{r['summary']:<12}\")\n",
    "        \n",
    "        # Find best method\n",
    "        best_method = max(results.items(), \n",
    "                         key=lambda x: x[1]['exact_match_rate'])\n",
    "        print(f\"\\nBest performing method: {best_method[0].upper()}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results):\n",
    "        \"\"\"\n",
    "        Print evaluation results.\n",
    "        \"\"\"\n",
    "        print(f\"Execution Success Rate: {results['execution_success_rate']:.1%}\")\n",
    "        print(f\"Symbol Accuracy: {results['avg_symbol_accuracy']:.1%}\")\n",
    "        print(f\"Exact Match Rate: {results['exact_match_rate']:.1%}\")\n",
    "        print(f\"Quality Score: {results['quality_score']:.1%}\")\n",
    "        print(f\"Assessment: {results['summary']}\")\n",
    "        \n",
    "        # Show problematic equations\n",
    "        problematic = [eq for eq in results['equation_details'] \n",
    "                      if not eq['equivalent']]\n",
    "        if problematic:\n",
    "            print(f\"Problematic equations: {[eq['index'] for eq in problematic[:5]]}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "def run_evaluation(correct_odes_sorted, extracted_odes_sorted, \n",
    "                  corrected_odes_sorted, mtx_odes_sorted, biomodel_name):\n",
    "    \"\"\"\n",
    "    Run the evaluation for your ODEs.\n",
    "    \"\"\"\n",
    "    evaluator = ODEsEvaluator()\n",
    "    \n",
    "    # Run comparison\n",
    "    results = evaluator.compare_all_methods(\n",
    "        correct_odes_sorted,\n",
    "        extracted_odes_sorted,\n",
    "        corrected_odes_sorted,\n",
    "        mtx_odes_sorted\n",
    "    )\n",
    "    \n",
    "    # Save results to file\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_file = f\"evaluation_{biomodel_name}_{timestamp}.txt\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Evaluation Results for {biomodel_name}\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\\n\")\n",
    "        \n",
    "        for method, res in results.items():\n",
    "            f.write(f\"\\n{method.upper()} METHOD:\\n\")\n",
    "            f.write(f\"  Execution Success: {res['execution_success_rate']:.1%}\\n\")\n",
    "            f.write(f\"  Exact Match Rate: {res['exact_match_rate']:.1%}\\n\")\n",
    "            f.write(f\"  Quality Score: {res['quality_score']:.1%}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage with your variables\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming your variables are already defined:\n",
    "    # correct_odes_sorted, extracted_odes_sorted, corrected_odes_sorted, mtx_odes_sorted\n",
    "    \n",
    "    evaluator = ODEsEvaluator()\n",
    "    results = evaluator.compare_all_methods(\n",
    "        correct_odes_sorted,\n",
    "        extracted_odes_sorted,\n",
    "        corrected_odes_sorted,\n",
    "        mtx_odes_sorted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_agent_pipeline(\n",
    "    image_path: str,\n",
    "    client: OpenAIClient,\n",
    "    verbose: bool = True\n",
    ") -> tuple[str, Optional[dict], dict]:\n",
    "    \"\"\"\n",
    "    Multi-agent pipeline for ODE extraction and validation\n",
    "    \n",
    "    Phase 1: Extract ODEs from image\n",
    "    Phase 2: Fix execution errors\n",
    "    Phase 3: Validate and correct (parallel checks)\n",
    "    Phase 4: Evaluate quality\n",
    "    \n",
    "    Returns:\n",
    "        Validated ODE string, concepts, and quality score\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(\"MULTI-AGENT ODE EXTRACTION & VALIDATION PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Extraction (part of pipeline for context tracking)\n",
    "    ode_str, concepts = phase1_extract_odes(image_path, client, verbose)\n",
    "    \n",
    "    # Phase 2: Execution error correction\n",
    "    ode_str = phase2_fix_execution_errors(ode_str, client, verbose)\n",
    "    \n",
    "    # Phase 3: Validation and mathematical correction\n",
    "    ode_str, concepts = phase3_validate_and_correct(ode_str, concepts, client, verbose)\n",
    "    \n",
    "    # Phase 4: Quality evaluation\n",
    "    quality_score = phase4_evaluate_quality(ode_str, concepts, {}, client, verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PIPELINE COMPLETE - Quality Score: {quality_score['total_score']:.2%}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return ode_str, concepts, quality_score\n",
    "\n",
    "# Individual phase functions\n",
    "def phase1_extract_odes(\n",
    "    image_path: str, \n",
    "    client: OpenAIClient,\n",
    "    verbose: bool = True\n",
    ") -> tuple[str, Optional[dict]]:\n",
    "    \"\"\"Phase 1: Extract ODEs and concepts from image\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\nPHASE 1: ODE Extraction\")\n",
    "    \n",
    "    # Use existing extraction functions\n",
    "    ode_str = image_file_to_odes_str(image_path, client)\n",
    "    \n",
    "    try:\n",
    "        concepts = get_concepts_from_odes(ode_str, client)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"  Warning: Concept extraction failed: {e}\")\n",
    "        concepts = None\n",
    "    \n",
    "    return ode_str, concepts\n",
    "\n",
    "def phase2_fix_execution_errors(\n",
    "    ode_str: str, \n",
    "    client: OpenAIClient,\n",
    "    verbose: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Phase 2: Check and fix execution errors\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\nPHASE 2: Execution Error Check & Correction\")\n",
    "    \n",
    "    from agents import ExecutionErrorCorrector\n",
    "    corrector = ExecutionErrorCorrector(client)\n",
    "    result = corrector.process({'ode_str': ode_str})\n",
    "    \n",
    "    if verbose and result.get('execution_report', {}).get('errors_fixed'):\n",
    "        print(f\"  Fixed {len(result['execution_report']['errors_fixed'])} errors\")\n",
    "    \n",
    "    return result['ode_str']\n",
    "\n",
    "def phase3_validate_and_correct(\n",
    "    ode_str: str, \n",
    "    concepts: Optional[dict],\n",
    "    client: OpenAIClient,\n",
    "    verbose: bool = True\n",
    ") -> tuple[str, Optional[dict]]:\n",
    "    \"\"\"Phase 3: Validation and mathematical checks with corrections\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\nPHASE 3: Validation & Mathematical Checks\")\n",
    "    \n",
    "    from agents import (\n",
    "        ValidationAggregator,\n",
    "        MathematicalAggregator,\n",
    "        UnifiedErrorCorrector\n",
    "    )\n",
    "    \n",
    "    pipeline_state = {'ode_str': ode_str, 'concepts': concepts}\n",
    "    \n",
    "    # Run parallel validation checks\n",
    "    val_aggregator = ValidationAggregator(client)\n",
    "    val_results = val_aggregator.process(pipeline_state)\n",
    "    \n",
    "    math_aggregator = MathematicalAggregator(client)\n",
    "    math_results = math_aggregator.process(pipeline_state)\n",
    "    \n",
    "    # Apply unified corrections\n",
    "    pipeline_state.update(val_results)\n",
    "    pipeline_state.update(math_results)\n",
    "    \n",
    "    corrector = UnifiedErrorCorrector(client)\n",
    "    correction_result = corrector.process(pipeline_state)\n",
    "    \n",
    "    return correction_result['ode_str'], correction_result.get('concepts', concepts)\n",
    "\n",
    "def phase4_evaluate_quality(\n",
    "    ode_str: str,\n",
    "    concepts: Optional[dict],\n",
    "    reports: dict,\n",
    "    client: OpenAIClient,\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"Phase 4: Quantitative evaluation of extraction quality\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\nPHASE 4: Quantitative Evaluation\")\n",
    "    \n",
    "    from agents import QuantitativeEvaluator\n",
    "    evaluator = QuantitativeEvaluator(client)\n",
    "    \n",
    "    result = evaluator.process({\n",
    "        'ode_str': ode_str,\n",
    "        'concepts': concepts,\n",
    "        **reports\n",
    "    })\n",
    "    \n",
    "    return result['quality_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_template_model_from_sympy_odes(\n",
    "    ode_str,\n",
    "    attempt_grounding: bool,\n",
    "    client: OpenAIClient,\n",
    ") -> TemplateModel:\n",
    "    \"\"\"Create a TemplateModel from the sympy ODEs defined in the code snippet string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ode_str :\n",
    "        The code snippet defining the ODEs\n",
    "    attempt_grounding :\n",
    "        Whether to attempt grounding the concepts in the ODEs. This will prompt the\n",
    "        OpenAI chat completion to create concepts data to provide grounding for the\n",
    "        concepts in the ODEs. The concepts data is then used to create the TemplateModel.\n",
    "    client :\n",
    "        The OpenAI client\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :\n",
    "        The TemplateModel created from the sympy ODEs.\n",
    "    \"\"\"\n",
    "    # FixMe, for now use `exec` on the code, but need to find a safer way to execute\n",
    "    #  the code\n",
    "    # Import sympy just in case the code snippet does not import it\n",
    "    import sympy\n",
    "    odes: List[sympy.Eq] = None\n",
    "    # Execute the code and expose the `odes` variable to the local scope\n",
    "    local_dict = locals()\n",
    "    try:\n",
    "        exec(ode_str, globals(), local_dict)\n",
    "    except Exception as e:\n",
    "        # Raise a CodeExecutionError to handle the error in the UI\n",
    "        raise CodeExecutionError(f\"Error while executing the code: {e}\")\n",
    "    # `odes` should now be defined in the local scope\n",
    "    odes = local_dict.get(\"odes\")\n",
    "    assert odes is not None, \"The code should define a variable called `odes`\"\n",
    "    if attempt_grounding:\n",
    "        concept_data = get_concepts_from_odes(ode_str, client)\n",
    "    else:\n",
    "        concept_data = None\n",
    "    return template_model_from_sympy_odes(odes, concept_data=concept_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
